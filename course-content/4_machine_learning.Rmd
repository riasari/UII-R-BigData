---
title: "Machine Learning"
output: html_notebook
---

Load spark

```{r}
library(sparklyr)
library(dplyr)
library(ggplot2)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "8G"
spark <- spark_connect(master = "local", config = conf)
```

sparklyr provides bindings to Sparkâ€™s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package.

https://spark.rstudio.com/mlib/ outlines a list of supported algorithms.


```{r}
iris_tbl <- copy_to(spark, iris, "iris", overwrite = TRUE)
head(iris_tbl)

kmeans_model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(~ ., k = 3)

kmeans_model

# predict the associated class
predicted <- ml_predict(kmeans_model, iris_tbl) %>%
  collect
base::table(predicted$Species, predicted$prediction)

# plot cluster membership
ml_predict(kmeans_model) %>%
  collect() %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
             size = 2, alpha = 0.5) + 
  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
             col = scales::muted(c("red", "green", "blue")),
             pch = 'x', size = 12) +
  scale_color_discrete(name = "Predicted Cluster",
                       labels = paste("Cluster", 1:3)) +
  labs(
    x = "Petal Length",
    y = "Petal Width",
    title = "K-Means Clustering",
    subtitle = "Use Spark.ML to predict cluster membership with the iris dataset."
  )
```

> Spark is just a tool to run machine learning models on larger data!
Make sure to understand the concepts of machine learning without spark first.

There are many more machine learning models out there which are not supported by spark. But as shown in section UADF it is quite simple to parallelize these using sparklyR.

## lab task
Many more algorithms can be explored at https://spark.rstudio.com/mlib/ go through them as a lab exercise. Also look at https://spark.rstudio.com/graphframes/

## cleanup

Finally, close the spark session again.
```{r}
spark_disconnect(spark)
```
