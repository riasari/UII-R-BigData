---
title: "Streaming advanced"
output: html_notebook
---

Watermarked Window Aggregations and Joins

## Load spark
```{r}
library(sparklyr)
library(dplyr)
library(future)
# library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "8G"
# conf$`spark.sql.shuffle.partitions` <- "3"
# conf$`spark.sql.shuffle.partitions` <- "200"
# Enable kryo to decrease serialization overhead
conf$spark.serializer <- "org.apache.spark.serializer.KryoSerializer"
spark <- spark_connect(master = "local", config = conf)
```

## generate some dummy data

```{r}
source <- "streaming_source"
destination <- "streaming_destination"
if(file.exists(source)) unlink(source, TRUE)
if(file.exists(destination)) unlink(destination, TRUE)


lenght_df <- 1000
dates <- base::sample(seq(as.Date('2018-01-01'), as.Date('2019-06-01'), by="day"), replace = TRUE, lenght_df)
values <- rnorm(lenght_df)

event_category <- base::sample(c("regular", "error", "security_alert"), lenght_df, replace = TRUE)
sampling_df <- data.frame(values, dates, event_category)
sampling_df <- sampling_df %>%
  rename(timestamp = dates) %>%
  rename(category = event_category) %>%
  rename(value = values)
head(sampling_df)

stream_generate_test(df = sampling_df, iterations = 1, path = source)
```


## Window Operations on Event Time

Aggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations.

![illustration of above query](img/structured-streaming-window.png)

**performance considerations**

It will take a surprisingly long time for the previous query to start generating data (on a real spark cluster; when only running locally sparklyR automatically set's it up correctly) .

It's our `groupBy()`. `groupBy()` causes a shuffle, and, by default, Spark SQL shuffles to 200 partitions. In addition, we're doing a *stateful* aggregation: one that requires Structured Streaming to maintain and aggregate data over time.

One way to reduce this overhead is to reduce the number of partitions Spark shuffles to:
```{r}
spark_session_config(spark, "spark.sql.shuffle.partitions", 2L)
# spark_session_config(spark, "spark.sql.shuffle.partitions", 200L)
```
It is left as an exercise to the reader to experiment with different levels of shuffle paralellism.

There is a another problem: to run this query for days, itâ€™s necessary for the system to bound the amount of intermediate in-memory state it accumulates. Every window has to be separately persisted and maintained. Over time, this data will build up, slowing things down.

It can be mitigated when selecting a coarser window (instead of seconds minutes or days) but eventually even this will run out of resources.

Instead **watermarking** is a better solution:
Watermarking has been introduced just for this.
In R `stream_watermark(df, column="timestamp", threshold="14 days")` enables Watermarking.

`window(timestamp,  "7 days", "3 days")` which defines a sliding window over the timestamp (watermark) column which has a size of 7 days (any data later than the watermark is dropped, within this time, the window is updated) and and slides over every 3 days.
There *is later than the watermark* means:
For a specific window starting at time $T$, Spark will maintain state and allow late data to update the state until (*max event time* seen by Spark - *late threshold*) exceeds $T$, i.e. is older than 14 days.

![watermarking](img/structured-streaming-watermark-update-mode.png)

Example code:

```{r}
read_folder <- stream_read_csv(spark, source) 

process_stream <- read_folder %>%
  stream_watermark(column="timestamp", threshold="14 days") %>% 
  group_by(time_window = window(timestamp,  "7 days", "3 days"), category) %>%
  summarise(
    mean = mean(value, na.rm = TRUE),
    count = n()
  ) %>%
  sdf_separate_column("time_window", into=c("beginning", "end")) %>%
  select(-time_window) %>%
  arrange(desc(count))

my_table <- "stream"
write_output <- stream_write_memory(process_stream, name = my_table)
##########################################
tbl(spark, my_table)  # run once
##########################################

invisible(future(stream_generate_test(df = sampling_df, interval = 0.2, iterations = 100, path = source)))

##########################################
tbl(spark, my_table)  # execute repeatedly
##########################################
#stream_view(write_output)
```




### stream-stream JOINs

As of Spark 2.3 also Stream-stream and not only stream-static joins are available.

> NOTE: this is still rather experimental and not well supported in the `dbplyr` tooling. Falling back to executing textual `SQL` is required for now. See https://github.com/rstudio/sparklyr/issues/1977 for details.

#### sample data generation

```{r}
source_1 <- "streaming_source"
destination_1 <- "streaming_destination"
source_2 <- "streaming_source_2"
destination_2 <- "streaming_destination_2"
if(file.exists(source_1)) unlink(source_1, TRUE)
if(file.exists(destination_1)) unlink(destination_1, TRUE)
if(file.exists(source_2)) unlink(source_2, TRUE)
if(file.exists(destination_2)) unlink(destination_2, TRUE)

######################### first stream #############
lenght_df <- 1000
dates <- base::sample(seq(as.Date('2018-01-01'), as.Date('2019-06-01'), by="day"), replace = TRUE, lenght_df)
values <- rnorm(lenght_df)
event_category <- base::sample(c("regular", "error", "security_alert"), lenght_df, replace = TRUE)
sampling_df <- data.frame(values, dates, event_category)
sampling_df <- sampling_df %>%
  rename(timestamp = dates) %>%
  rename(category = event_category) %>%
  rename(value = values)
head(sampling_df)

stream_generate_test(df = sampling_df, iterations = 1, path = source_1)

######################### second stream #############
dates <- base::sample(seq(as.Date('2018-01-01'), as.Date('2019-06-01'), by="day"), replace = TRUE, lenght_df)
second_values <- rnorm(lenght_df)
second_category <- base::sample(c("regular", "error", "security_alert"), lenght_df, replace = TRUE)
sampling_df_2 <- data.frame(second_values, dates, second_category)
sampling_df_2 <- sampling_df_2 %>%
  rename(timestamp = dates)
head(sampling_df_2)

stream_generate_test(df = sampling_df_2, iterations = 1, path = source_2)
```

join execution

> TODO figure it out: https://github.com/rstudio/sparklyr/issues/1977

```{r}
read_folder_1 <- stream_read_csv(spark, source_1) %>%
  stream_watermark(column="timestamp", threshold="17 days")
read_folder_2 <- stream_read_csv(spark, source_2)%>%
  stream_watermark(column="timestamp", threshold="7 days")

# How can windows be added into join even though sparklyR seems to muss them?
#expr("""
#    clickAdId = impressionAdId AND
#    clickTime >= impressionTime AND
#    clickTime <= impressionTime + interval 1 hour
#    """)

# ERROR: this one fails (bug in sparklyr that the watermarks are not picket up correctly)
# Stream-stream outer join between two streaming DataFrame/Datasets is not supported without a watermark in the join keys, or a watermark on the nullable side and an appropriate range condition
#joined_streams <- read_folder_1 %>% left_join(read_folder_2, by = c("category"= "second_category"))

# this one works
joined_streams <- read_folder_1 %>% inner_join(read_folder_2, by = c("category"= "second_category"))

my_table <- "stream"
stream_write_memory(joined_streams, name = my_table)

tbl(spark, my_table)  # execute repeatedly

################# TODO add time window into join and watermarks
invisible(future(stream_generate_test(df = sampling_df_1, interval = 0.2, iterations = 100, path = source_1)))
invisible(future(stream_generate_test(df = sampling_df_2, interval = 0.5, iterations = 200, path = source_2)))

tbl(spark, my_table)  # execute repeatedly
```

Joining Stream with Batch dataset works fine.
Joining a Stream with another stream and applying an inner join also works fine.
But for a left (outer) join on two streams sparklyR somehow messes up the watermarking and the join does not work.

### Triggers

- unspecified. Generates a new microbatch after the last one has finished.
- fixed micro batches, in R `stream_trigger_interval`
- run once https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.ht
- continous (alpha)

```{r}
stream_trigger_interval(interval = 1000)  # fixed execution interval in milliseconds.
```
Any other trigger s (run-once, continous) seem to not be supported yet by the R interfaces to spark.


Finally cleanup / stop butting in RStudio:
```{r}
stream_stop(write_output) # cleanup
```

## cleanup

Finally, close the spark session again.
```{r}
spark_disconnect(spark)
```