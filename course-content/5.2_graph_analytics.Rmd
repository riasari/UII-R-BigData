---
title: "Graph Analytics"
output:
  html_document:
    df_print: paged
---

A graph contains nodes and edges.
Brief theoretic introduction to graphs:

- https://www.analyticsvidhya.com/blog/2018/04/introduction-to-graph-theory-network-analysis-python-codes/
- https://www.freecodecamp.org/news/i-dont-understand-graph-theory-1c96572a1401/

Install the package

```{r}
devtools::install_github("rstudio/graphframes")
install.package("ggraph")
```


```{r}
library(ggraph)
library(graphframes)
library(sparklyr)
library(dplyr)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "8G"

spark <- spark_connect(master = "local", config = conf)
```

load a dummy graph

```{r}
highschool_tbl <- copy_to(spark, ggraph::highschool, "highschool")

highschool_tbl
```

The vertices table is be constructed using `dplyr`. The variable name expected by the GraphFrame is `id`.
Manually constructing nodes and edges tables:

```{r}
highschool_tbl <- copy_to(spark, ggraph::highschool, "highschool", overwrite = TRUE) %>%
  filter(year == 1957) %>%
  transmute(from = as.character(as.integer(from)),
            to = as.character(as.integer(to)))

from_tbl <- highschool_tbl %>%
  distinct(from) %>%
  transmute(id = from)
to_tbl <- highschool_tbl %>%
  distinct(to) %>%
  transmute(id = to)

vertices_tbl <- distinct(sdf_bind_rows(from_tbl, to_tbl))
edges_tbl <- highschool_tbl %>%
  transmute(src = from, dst = to)

vertices_tbl

edges_tbl

graph <- gf_graphframe(vertices_tbl, edges_tbl)

graph

gf_degrees(graph) %>% summarise(friends = mean(degree))


gf_shortest_paths(graph, 33) %>%
  filter(size(distances) > 0) %>%
  mutate(distance = explode(map_values(distances))) %>%
  select(id, distance)

gf_graphframe(vertices_tbl, edges_tbl) %>%
  gf_pagerank(reset_prob = 0.15, max_iter = 10L)

highschool_tbl %>%
  igraph::graph_from_data_frame(directed = FALSE) %>%
  ggraph(layout = 'kk') + 
    geom_edge_link(alpha = 0.2,
                   arrow = arrow(length = unit(2, 'mm')),
                   end_cap = circle(2, 'mm'),
                   start_cap = circle(2, 'mm')) + 
    geom_node_point(size = 2, alpha = 0.4)
```


## cleanup

Finally, close the spark session again.
```{r}
spark_disconnect(spark)
```
