---
title: "Introduction to DataFrames"
output: html_notebook
---

Load spark

```{r}
library(sparklyr)
library(dplyr)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "8G"
spark <- spark_connect(master = "local", config = conf)
```

One hour of Pagecounts from the Wikimedia projects captured March 1st, 2019, at 12:00 PM UTC. It can be obtained from https://dumps.wikimedia.org/other/pageviews/2019/2019-03/ and documentatin is found at: https://dumps.wikimedia.org/other/pageviews/readme.html

- Filetype: gzip compressed parquet
- size 80MB

## Create a DataFrame
Read the Parquet files into a DataFrame.

```{r}
pagecountsAllDF <- spark_read_parquet(spark, "datasets/wikimedia_edits.parquet")
```

Look to the right side of RStudio and use the Spark pane in the connections tab to examine the dataset.

## counting records
The `count()` action:
> Returns the number of rows in the Dataset.

`count()` will trigger a job to process the request and return a value.

To count all records in our `DataFrame` execute:

```{r}
sdf_nrow(pagecountsAllDF)
```

That tells us that there are around $6$ million rows.

Before taking a closer look let's introduce a technique that speeds up processing.

## `cache` and `persist``
Iterative algorithms, i.e. when a dataframe is used at least twice can greatly improve performance when caching intermediate results in memory or persisting to disk.

```{r}
tbl_cache(spark, "wikimedia_edits", force=FALSE)
# now go to: http://localhost:4040/storage/
sdf_nrow(pagecountsAllDF)
         # cache the dataframe
         # maaterialize
```

Run the `count` command again. It should be faster now. Again, look at the spark UI and see what is different in the execution graph.

```{r}
sdf_nrow(pagecountsAllDF)
```

To remove a table from the chace use: `sparklyr::tbl_uncache()`, but this is not necessary now.

## the data
`printSchema` shows the schema of the data, i.e. the data types for each column.

```{r}
sdf_schema(pagecountsAllDF)
```

Show unique values per project.

```{r}
pagecountsAllDF %>%
  select(project) %>%
    distinct()
```

When we also want to count how many unique values are present:

```{r}
pagecountsAllDF %>%
  group_by(project)%>%
  summarise(count_per_project = count())
```

When counting for unique number of articles per project, an approximative solution might be faster:

```{r}
# dplyr exact
pagecountsAllDF %>%
  group_by(project)%>%
  summarise(count_unique = n_distinct(article))

# dplyr approx
pagecountsAllDF %>%
  group_by(project)%>%
  summarise(count_unique = approx_count_distinct(article))
```

Watch the results closely and examine how good they lign up. Note, there is a tunable accuracy for the approximate method.

## Spark API Documentation

Find it here https://spark.apache.org/docs/latest/. Other Documentation:

- Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...
- Deployment Guides for Spark Standalone, Mesos, Yarn...
- Configuration, Monitoring, Tuning, Security...

There is a different API and corresponding documentation for each language:
- [scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)
- [java](https://spark.apache.org/docs/latest/api/java/index.html)
- [python](https://spark.apache.org/docs/latest/api/python/index.html)
- [R](https://spark.apache.org/docs/latest/api/R/index.html) for sparkR
- [sparklyR](https://spark.rstudio.com/)
- [SQL](https://spark.apache.org/docs/latest/api/sql/index.html) also helpful in case a function is missing in the sparklyR API

## show

Spark usually differenciates between `show()`, `take(n)` and `collect()` which either show the first couple of rows to standard out, return the first n records as an array or collect the entire Dataframe into the memory of the Driver. Be careful with `collect` as it can easily kill the driver node with out of memory exception unless the data is filtered or aggregated first.


With sparklyR a simple:

```{r}
pagecountsAllDF

# be careful with what you collect locally
local_df <- sparklyr::collect(pagecountsAllDF)
```
is enough as the tibble is automatically reduced in the output a paginated table for the first couple of 1000 records.

## transformation vs. action

Here, we use the filter function (similar to SQL `WHERE`) to keep only rows matching our kriterion.

```{r}
transformedDf <- pagecountsAllDF %>%
  filter(requests == 1)
```

Notice how "nothing" happened -  no job was triggered.
We are simply defining the next step in our transformations.
It's not until we induce an action that a job is triggered and the data is processed.


```{r}
transformedDf
```

## slect

If you do not want to look at all the columns use `select()` 

```{r}
pagecountsAllDF %>%
    select(requests, bytes_served) 
```

It is one of the most powerful and commonly used transformations. `drop()` works similarly, but instead of keeping, removes the staed columns:

```{r}
pagecountsAllDF %>%
  select(-requests, -article)
```

## distinct and drop duplicates

Removes duplicates.
> WARNING ordering of records is important if you care about which duplicate is kept. Posibly a different column (time) might have an impact. To reliably deduplicate in such a case you should use a window function with rank.

```{r}
pagecountsAllDF %>%
  distinct(requests)
```

## SQL temporary view

If you want to execute plain SQL this is also really easy:

```{sql connection=spark}
SELECT * FROM wikimedia_edits LIMIT 2
```

## LAB 1
go to Lab 1 and try it yourself

## orderBy

Looking at the data it is apparent that there is no reason to assume the data is sorted on any of the columns.

To answer:
*What are the top 10 most requested articles?*
explicit ordering can be imposed:

```{r}
pagecountsAllDF %>% 
  arrange(requests)
```

This sorts the data, but in the wrong direction. We want to sort in descending order:

```{r}
pagecountsAllDF %>% 
  arrange(desc(requests))
```

And there are many more useful functions. Be sure to check out https://spark.rstudio.com/dplyr/

## window functions
Another useful kind of function are window functions

Let's work on another dataset. This time we copy an existing dataframe from R to spark.
```{r}
library(nycflights13)
flights <- copy_to(spark, flights, "flights")
airlines <- copy_to(spark, airlines, "airlines")
```

Now using a specific ordering per group it is easy to identify the first and last records (rank them) to figure out the most and least delayed flight per day

```{r}
bestworst <- flights %>%
  group_by(year, month, day) %>%
  select(dep_delay) %>% 
  filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))
```

to debug we can have a look at the generate SQL statements:

```{r}
dbplyr::sql_render(bestworst)
```

Notice, no actual computation has been performed until now. The results are:

```{r}
bestworst
```

## JOINs

Spark supports various JOINs. Even non-equi-joins are possible.
![various JOINs](img/join-types.png)
```{r}
flights %>% left_join(airlines)
```

## writing data

Spark supports reading and writing data from and to various systems and file formats.
![swiss army knife](img/ecosystem.png)
Writing to parquet, especially if gzip compressed still can be useful even for medium sized local files. The compression can be impressive.

```{r}
spark_write_parquet(flights, "datasets/flights" , mode="overwrite", options = list(compression="gzip"))
```

Notice the difference:
```{r}
spark_write_parquet(flights, "datasets/flights" ,partition_by = "carrier", mode="overwrite", options = list(compression="gzip"))
```

This command writes the data using partitioning.
In the Hadoop world, this commonly is referred to partition pruning. When querying for a specific value of the partitioning column, only the matching files will be read into memory.

See the result in our case:

```{bash}
ls datasets/flights
```

Optimizing for the common case can greatly influence the speed of analytical queries.

## LAB 1
go to Lab 1 and try it yourself

## cleanup

Finally, close the spark session again.
```{r}
spark_disconnect(spark)
```
